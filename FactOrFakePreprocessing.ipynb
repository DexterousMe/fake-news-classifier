{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier\n",
    " \n",
    "Data : https://www.kaggle.com/jruvika/fake-news-detection/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide deprecated warnings of sklearn package\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake Articles ->  (2120, 4)\n",
      "Number of Real Articles ->  (1868, 4)\n",
      "Maximum Length  32767\n",
      "Avg Length 2941.288365095286\n"
     ]
    }
   ],
   "source": [
    "# Drop the data with null or undefined values\n",
    "df = df.dropna()\n",
    "# Get the number of each label in the data\n",
    "fake = df[df.Label ==  0]\n",
    "real = df[df.Label ==  1]\n",
    "\n",
    "print('Number of Fake Articles -> ', fake.shape)\n",
    "print('Number of Real Articles -> ', real.shape)\n",
    "\n",
    "# Max Count of words in Document\n",
    "max = 0\n",
    "total = 0\n",
    "count = 0;\n",
    "for i in range(df.shape[0]):\n",
    "    length = len(df.iloc[i,2])\n",
    "    total += length\n",
    "    count += 1\n",
    "    if(length > max):\n",
    "        max = length\n",
    "    \n",
    "print(\"Maximum Length \", max)\n",
    "print(\"Avg Length\", total / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Using Stratified sampling, split the data into 70-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake articles in Training set ->  1696\n",
      "Number of Real articles in Training set ->  1494\n",
      "Number of Fake articles in Testing set ->  424\n",
      "Number of Real articles in Testing set ->  374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.pop('Label')\n",
    "x = df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "train_count = y_train.value_counts()\n",
    "test_count = y_test.value_counts()\n",
    "\n",
    "print('Number of Fake articles in Training set -> ', train_count[0])\n",
    "print('Number of Real articles in Training set -> ', train_count[1])\n",
    "print('Number of Fake articles in Testing set -> ', test_count[0])\n",
    "print('Number of Real articles in Testing set -> ', test_count[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "    * Lowercase all the words\n",
    "    * Negation Handling\n",
    "    * Remove Stopwords\n",
    "    * Remove punctuations and Empty Strings from the array\n",
    "* Stemming\n",
    "\n",
    "Source - https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "\n",
    "#### Setup:\n",
    "\n",
    "* Import NLTK\n",
    "* Download and import stopwords from NLTK.corpus\n",
    "* Import PorterStemmer which is the module used for stemming\n",
    "* Import NLTK Vader Sentiment Analysis Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK library and its needed modules\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Import the Vader Sentiment Analysis Library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the Apostrophes connecting words\n",
    "appos_file = open('appos.txt','r')\n",
    "appos = eval(appos_file.read())\n",
    "appos_file.close()\n",
    "\n",
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "# Function to preprocess a single article\n",
    "# Document refers to the text of the Article.\n",
    "def processDocument(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "        words = map(str.lower, words)\n",
    "        \n",
    "        # Negation Handling map is'nt to is not : \n",
    "        words = map(lambda x: negationHandling(x), words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "        \n",
    "        # stemming\n",
    "        words = map(lambda x: ps.stem(x), words)\n",
    "        \n",
    "        # Adding the preprocessed words to the document\n",
    "        tokens = tokens + list(words)\n",
    "        \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of training data\n",
      "Preprocessing Completed for HeadLine of training data\n"
     ]
    }
   ],
   "source": [
    "# Processing the body i.e. text of the Article\n",
    "train_body = x_train.loc[:,'Body']\n",
    "train_raw_body = []\n",
    "train_body_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_body_sentiment.append(list(analyser.polarity_scores(train_body.iloc[i]).values()))\n",
    "    train_raw_body.append(train_body.iloc[i])\n",
    "        \n",
    "train_body_wordArray = list(map(lambda x: processDocument(x), train_raw_body))\n",
    "print(\"Preprocessing Completed for Body of training data\")\n",
    "\n",
    "# Process the Headlines of the training data.\n",
    "train_headline = x_train.loc[:,'Headline']\n",
    "train_raw_headline = []\n",
    "train_headline_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment.append(list(analyser.polarity_scores(train_headline.iloc[i]).values()))\n",
    "    train_raw_headline.append(train_headline.iloc[i])\n",
    "        \n",
    "train_headline_wordArray = list(map(lambda x: processDocument(x), train_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec Model\n",
      "Training iteration 10\n",
      "Training iteration 20\n",
      "Training iteration 30\n",
      "Training iteration 40\n",
      "Training iteration 50\n",
      "Training iteration 60\n",
      "Training iteration 70\n",
      "Training iteration 80\n",
      "Training iteration 90\n",
      "Training iteration 100\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_body_data = [TaggedDocument(\n",
    "    words = train_body_wordArray[i], \n",
    "    tags = [str(i)]) for i, _d in enumerate(train_body_wordArray)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size = vec_size,\n",
    "    alpha = alpha, \n",
    "    min_alpha = 0.025,\n",
    "    min_count = 5,\n",
    "    window = 10,\n",
    "    dm = 1)\n",
    "\n",
    "model.build_vocab(tagged_body_data)\n",
    "print('Training Doc2Vec Model')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if ((epoch + 1) % 10 == 0):\n",
    "        print('Training iteration {0}'.format(epoch + 1))\n",
    "    model.train(tagged_body_data,\n",
    "                total_examples = model.corpus_count,\n",
    "                epochs = model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of testing data\n",
      "Preprocessing Completed for HeadLine of testing data\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the body of the article of test Data set \n",
    "test_body = x_test.loc[:,'Body']\n",
    "test_raw_body = []\n",
    "test_body_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_body_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_body.append(test_body.iloc[i])\n",
    "        \n",
    "test_body_wordArray = list(map(lambda x: processDocument(x), test_raw_body))\n",
    "print(\"Preprocessing Completed for Body of testing data\")\n",
    "\n",
    "# Preprocess the Headline of the article for testing dataset\n",
    "test_headLine = x_test.loc[:,'Headline']\n",
    "test_raw_headline = []\n",
    "test_headline_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_headline.append(test_headLine.iloc[i])\n",
    "        \n",
    "test_headline_wordArray = list(map(lambda x: processDocument(x), test_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Contains the list of Fake News website URL's\n",
    "fake_URL_df = pd.read_csv(\"Data/fake_news_websites.csv\")\n",
    "# Contains the list of Authentic News website URL's\n",
    "fact_URL_df = pd.read_csv(\"Data/fact_news_websites.csv\")\n",
    "\n",
    "##Computes the similarity score between 2 strings\n",
    "def similarityRatio(url_one, url_two):\n",
    "    return SequenceMatcher(None, url_one, url_two).ratio()\n",
    "\n",
    "##Extracts the domain from a URL - for example: 'https://www.bbc.com' will become 'bbc'\n",
    "def getDomain(url):\n",
    "    return url.lstrip(\"https://www.\").split(\".\")[0]\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of fake websites\n",
    "def fakeURLCheckAssign(url):\n",
    "    minSimilarity = 0.5\n",
    "    for i in range(len(fake_URL_df['SiteName'])):\n",
    "        similarity = similarityRatio(getDomain(url),getDomain(fake_URL_df['SiteName'][i]))\n",
    "        if (similarity > 0.75):\n",
    "            minSimilarity = min(minSimilarity,(1 - similarity))\n",
    "            \n",
    "    return minSimilarity\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of authentic websites\n",
    "def factURLCheckAssign(url):\n",
    "    for i in range(len(fact_URL_df['SiteName'])):\n",
    "        minSimilarity = 0.5\n",
    "        similarity = similarityRatio(getDomain(url), getDomain(fact_URL_df['SiteName'][i]))\n",
    "\n",
    "        if similarity == 1:\n",
    "            return similarity\n",
    "            \n",
    "        if similarity > 0.75:\n",
    "            minSimilarity = min(minSimilarity,1 - similarity)\n",
    "            \n",
    "    return minSimilarity\n",
    "          \n",
    "def URLScore(url):\n",
    "    fakeScore = fakeURLCheckAssign(url)\n",
    "    factScore = factURLCheckAssign(url)\n",
    "    if factScore == 1:\n",
    "        return 1\n",
    "    if fakeScore == 0:\n",
    "        return 0\n",
    "    if factScore == 0.5 and fakeScore == 0.5:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return min(fakeScore, factScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_URL = x_test.loc[:,'URLs']\n",
    "train_URL = x_train.loc[:,'URLs']\n",
    "# Higher Score denotes that the article is more authentic\n",
    "# Completely Real Article URL Score - 1 & Completely Fake Article URL Score - 0\n",
    "\n",
    "# Training data set URL Score\n",
    "train_URLScore_vector = []    \n",
    "for i in range(x_train.shape[0]):\n",
    "    train_URLScore_vector.append(URLScore(train_URL.iloc[i]))\n",
    "\n",
    "# Testing data set URL Score\n",
    "test_URLScore_vector = []    \n",
    "for i in range(x_test.shape[0]):\n",
    "    test_URLScore_vector.append(URLScore(test_URL.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.125, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.125, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.125, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.125, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 1, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.125, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.5, 1, 0.125, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.125, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.5, 1, 1, 1, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.125, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.5, 1, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.5, 0.125, 0.5, 0.5, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.5, 1, 0.5, 1, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073]\n",
      "[1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.5, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.125, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.125, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.5, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.125, 0.125, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_URLScore_vector)\n",
    "print(test_URLScore_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word vectors using the trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the np training data (3190, 609)\n",
      "Shape of the np training data (798, 609)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# Training set Body Word Vector  \n",
    "train_body_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    trainBodyConcat = np.concatenate([model.docvecs[i], np.asarray(train_body_sentiment[i])])\n",
    "    train_body_vector.append(trainBodyConcat)\n",
    "\n",
    "# Training data set Headline Word Vectors\n",
    "train_headline_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment[i].append(train_URLScore_vector[i])\n",
    "    trainHeadConcat = np.concatenate([model.infer_vector(train_headline_wordArray[i]), \n",
    "                                      np.asarray(train_headline_sentiment[i])]) \n",
    "    train_headline_vector.append(trainHeadConcat)\n",
    "\n",
    "# Testing set Body Word Vector\n",
    "test_body_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    testBodyConcat = np.concatenate([model.infer_vector(test_body_wordArray[i]), \n",
    "                                     np.asarray(test_body_sentiment[i])])\n",
    "    test_body_vector.append(testBodyConcat)\n",
    "    \n",
    "# Testing set Headline Word Vectors\n",
    "test_headline_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment[i].append(test_URLScore_vector[i])\n",
    "    testHeadConcat = np.concatenate([model.infer_vector(test_headline_wordArray[i]), \n",
    "                                     np.asarray(test_headline_sentiment[i])])\n",
    "    test_headline_vector.append(testHeadConcat)    \n",
    "\n",
    "# Create Numpy Array for training data to train sklearn models\n",
    "np_train_headline = np.array([np.array(xi) for xi in train_headline_vector]) \n",
    "np_train_body = np.array([np.array(xi) for xi in train_body_vector])\n",
    "\n",
    "inp_x_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    inp_x_train.append(np.concatenate((np_train_headline[i], np_train_body[i])))\n",
    "\n",
    "inp_x_train = np.array(inp_x_train)\n",
    "\n",
    "# Create np Array for testing data to train sklearn models\n",
    "np_test_headline = np.array([np.array(xi) for xi in test_headline_vector])\n",
    "np_test_body = np.array([np.array(xi) for xi in test_body_vector])\n",
    "\n",
    "inp_x_test = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    inp_x_test.append(np.concatenate((np_test_headline[i], np_test_body[i])))\n",
    "\n",
    "inp_x_test = np.array(inp_x_test)\n",
    "\n",
    "print('Shape of the np training data', inp_x_train.shape)\n",
    "print('Shape of the np training data', inp_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "C = [0.1, 0.5, 1, 5, 10, 50]\n",
    "param_grid = [\n",
    "    {'C': C, 'kernel': ['linear']},\n",
    "    {'C': C, 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "    {'degree': [2,3,4], 'kernel': ['poly']},\n",
    "    {'coef0': [0.0], 'kernel': ['sigmoid']} \n",
    "]\n",
    "\n",
    "table = {}\n",
    "\n",
    "score_metric = 'accuracy'\n",
    "clf = GridSearchCV(svm.SVC(), param_grid, cv = 5, scoring = score_metric)\n",
    "clf.fit(inp_x_train, y_train)\n",
    "print(\"Best parameters set found :\", clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "\n",
    "for mean, params in zip(means, clf.cv_results_['params']):\n",
    "    if params == clf.best_params_:\n",
    "        print(\"%s -> %0.3f\" % (score_metric, mean))\n",
    "    key = str(params)\n",
    "    if key not in table:\n",
    "        table[key] = []\n",
    "    table[key].append(\"%0.3f\" % (mean))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Using GridSearchCV to find the optimal parameters for this training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SVC with above parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.7355889724310777\n",
      "Precision ->  0.823448384741381\n",
      "Recall ->  0.7355889724310777\n",
      "F-Score ->  0.7099835302088765\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "svm_model = svm.SVC(C = 10, gamma = 0.0001, kernel = 'rbf')\n",
    "svm_model.fit(inp_x_train, y_train)\n",
    "y_pred = svm_model.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.5588972431077694\n",
      "Precision ->  0.7395207981193294\n",
      "Recall ->  0.5588972431077694\n",
      "F-Score ->  0.42937990272280563\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(inp_x_train, y_train)\n",
    "\n",
    "nb_y_pred = gnb.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, nb_y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.9385964912280702\n",
      "Precision ->  0.9419162061509618\n",
      "Recall ->  0.9385964912280702\n",
      "F-Score ->  0.9386506099344967\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(inp_x_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_clf.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, dt_y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "#### Neural Network\n",
    "\n",
    "* Import Tensorflow\n",
    "* Import Keras\n",
    "* Set the seed value to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.2590 - acc: 0.5047\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2455 - acc: 0.5317\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.2251 - acc: 0.5395\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2013 - acc: 0.7771\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1826 - acc: 0.8235\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.1689 - acc: 0.8608\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.1581 - acc: 0.8815\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.1493 - acc: 0.8966\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.1418 - acc: 0.9100\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.1351 - acc: 0.9197\n",
      "798/798 [==============================] - 0s 41us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2491 - acc: 0.6473\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1679 - acc: 0.7715\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1189 - acc: 0.8502\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0901 - acc: 0.8925\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0733 - acc: 0.9154\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0617 - acc: 0.9323\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0544 - acc: 0.9429\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0482 - acc: 0.9539\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0439 - acc: 0.9571\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0400 - acc: 0.9605\n",
      "798/798 [==============================] - 0s 40us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.3131 - acc: 0.5060\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2324 - acc: 0.6423\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1892 - acc: 0.7241\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.1390 - acc: 0.8176\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1088 - acc: 0.8618\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0897 - acc: 0.8944\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0754 - acc: 0.9154\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0662 - acc: 0.9245\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0588 - acc: 0.9335\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0538 - acc: 0.9398\n",
      "798/798 [==============================] - 0s 37us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2109 - acc: 0.7044\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1124 - acc: 0.8517\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0767 - acc: 0.9091\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0601 - acc: 0.9345\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0503 - acc: 0.9498\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0428 - acc: 0.9580\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0372 - acc: 0.9643\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0331 - acc: 0.9712\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0298 - acc: 0.9755\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0271 - acc: 0.9765\n",
      "798/798 [==============================] - 0s 41us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.3027 - acc: 0.5705\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1607 - acc: 0.8063\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1061 - acc: 0.8777\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0791 - acc: 0.9103\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0641 - acc: 0.9295\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0536 - acc: 0.9429\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0460 - acc: 0.9527\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0403 - acc: 0.9614\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0362 - acc: 0.9677\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0332 - acc: 0.9708\n",
      "798/798 [==============================] - 0s 40us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2228 - acc: 0.6937\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1159 - acc: 0.8533\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0751 - acc: 0.9097\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0565 - acc: 0.9373\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0464 - acc: 0.9511\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0395 - acc: 0.9605\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0345 - acc: 0.9674\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0297 - acc: 0.9730\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0262 - acc: 0.9777\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0229 - acc: 0.9824\n",
      "798/798 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2600 - acc: 0.6439\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1292 - acc: 0.8354\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0809 - acc: 0.9025\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0607 - acc: 0.9323\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0502 - acc: 0.9458\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0419 - acc: 0.9520\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0360 - acc: 0.9683\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0315 - acc: 0.9724\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0278 - acc: 0.9755\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0247 - acc: 0.9812\n",
      "798/798 [==============================] - 0s 32us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2292 - acc: 0.7006\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1082 - acc: 0.8624\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0700 - acc: 0.9138\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0529 - acc: 0.9351\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0427 - acc: 0.9536\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0363 - acc: 0.9649\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0319 - acc: 0.9702\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0274 - acc: 0.9759\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0234 - acc: 0.9796\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0208 - acc: 0.9840\n",
      "798/798 [==============================] - 0s 38us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2365 - acc: 0.6605\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1086 - acc: 0.8658\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0749 - acc: 0.9157\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0585 - acc: 0.9354\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0488 - acc: 0.9489\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0422 - acc: 0.9592\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0364 - acc: 0.9661\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0322 - acc: 0.9718\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0289 - acc: 0.9746\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0256 - acc: 0.9784\n",
      "798/798 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2211 - acc: 0.6868\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1055 - acc: 0.8655\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0652 - acc: 0.9213\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0480 - acc: 0.9489\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0379 - acc: 0.9614\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0307 - acc: 0.9690\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0257 - acc: 0.9777\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0213 - acc: 0.9837\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0181 - acc: 0.9875\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0154 - acc: 0.9900\n",
      "798/798 [==============================] - 0s 32us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2099 - acc: 0.7135\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0971 - acc: 0.8787\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0657 - acc: 0.9210\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0503 - acc: 0.9464\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0401 - acc: 0.9586\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0343 - acc: 0.9687\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0304 - acc: 0.9708\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0255 - acc: 0.9781\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0218 - acc: 0.9831\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0196 - acc: 0.9856\n",
      "798/798 [==============================] - 0s 35us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2336 - acc: 0.6781\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0985 - acc: 0.8755\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0665 - acc: 0.9154\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0515 - acc: 0.9398\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0410 - acc: 0.9561\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0343 - acc: 0.9661\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0285 - acc: 0.9740\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0239 - acc: 0.9787\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0209 - acc: 0.9834\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0176 - acc: 0.9875\n",
      "798/798 [==============================] - 0s 32us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2140 - acc: 0.7041\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0918 - acc: 0.8875\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0631 - acc: 0.9257\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0491 - acc: 0.9448\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0392 - acc: 0.9592\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0330 - acc: 0.9690\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0283 - acc: 0.9752\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0248 - acc: 0.9777\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0218 - acc: 0.9815\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0186 - acc: 0.9853\n",
      "798/798 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2208 - acc: 0.7150\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0945 - acc: 0.8865\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0637 - acc: 0.9266\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0487 - acc: 0.9467\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0406 - acc: 0.9599\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0337 - acc: 0.9680\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0291 - acc: 0.9734\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0242 - acc: 0.9809\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0209 - acc: 0.9831\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0185 - acc: 0.9871\n",
      "798/798 [==============================] - 0s 35us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1907 - acc: 0.7433\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0835 - acc: 0.8962\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0574 - acc: 0.9288\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0418 - acc: 0.9533\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0315 - acc: 0.9680\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0261 - acc: 0.9774\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0210 - acc: 0.9815\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0181 - acc: 0.9884\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0151 - acc: 0.9893\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0130 - acc: 0.9922\n",
      "798/798 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2294 - acc: 0.6859\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1021 - acc: 0.8696\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0652 - acc: 0.9232\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0465 - acc: 0.9505\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0359 - acc: 0.9639\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0297 - acc: 0.9734\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0248 - acc: 0.9796\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0215 - acc: 0.9846\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0188 - acc: 0.9862\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0163 - acc: 0.9881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 36us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2313 - acc: 0.6718\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0992 - acc: 0.8730\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0657 - acc: 0.9191\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0497 - acc: 0.9445\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0405 - acc: 0.9561\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0333 - acc: 0.9680\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0276 - acc: 0.9749\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0237 - acc: 0.9806\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0197 - acc: 0.9856\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0168 - acc: 0.9884\n",
      "798/798 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2177 - acc: 0.6969\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0888 - acc: 0.8840\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0590 - acc: 0.9292\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0440 - acc: 0.9527\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0358 - acc: 0.9630\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0284 - acc: 0.9724\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0244 - acc: 0.9799\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0198 - acc: 0.9853\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0166 - acc: 0.9884\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0146 - acc: 0.9903\n",
      "798/798 [==============================] - 0s 32us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1966 - acc: 0.7426\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0816 - acc: 0.9000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0530 - acc: 0.9398\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0401 - acc: 0.9586\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0323 - acc: 0.9690\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0271 - acc: 0.9752\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0218 - acc: 0.9818\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0182 - acc: 0.9871\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0156 - acc: 0.9893\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0133 - acc: 0.9909\n",
      "798/798 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-297a835f532a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "# Set the seed as 7 to get reproducible results\n",
    "seed(7)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed as 7 here as well\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "train_accuracy = {}\n",
    "test_accuracy = {}\n",
    "models = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    no_of_hidden_neurons = i * 2\n",
    "    \n",
    "    # Create a new Model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(no_of_hidden_neurons, input_dim = inp_x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'mse',optimizer = 'adam',metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(inp_x_train, y_train, epochs = 10, batch_size = 100, verbose=2)\n",
    "    train_accuracy[no_of_hidden_neurons] = history.history['acc'][9]\n",
    "    \n",
    "    # Store the model\n",
    "    models.append(model)\n",
    "    \n",
    "    # Calculate the score on the testing data set\n",
    "    test_scores = model.evaluate(inp_x_test, y_test, batch_size = 100)\n",
    "    test_accuracy[no_of_hidden_neurons] = test_scores[1]\n",
    "    # Reset keras and tf\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "print(train_accuracy)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
