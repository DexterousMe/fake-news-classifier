{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier\n",
    " \n",
    "Data : https://www.kaggle.com/jruvika/fake-news-detection/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide deprecated warnings of sklearn package\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake Articles ->  (2120, 4)\n",
      "Number of Real Articles ->  (1868, 4)\n",
      "Maximum Length  32767\n",
      "Avg Length 2941.288365095286\n"
     ]
    }
   ],
   "source": [
    "# Drop the data with null or undefined values\n",
    "df = df.dropna()\n",
    "# Get the number of each label in the data\n",
    "fake = df[df.Label ==  0]\n",
    "real = df[df.Label ==  1]\n",
    "\n",
    "print('Number of Fake Articles -> ', fake.shape)\n",
    "print('Number of Real Articles -> ', real.shape)\n",
    "\n",
    "# Max Count of words in Document\n",
    "max = 0\n",
    "total = 0\n",
    "count = 0;\n",
    "for i in range(df.shape[0]):\n",
    "    length = len(df.iloc[i,2])\n",
    "    total += length\n",
    "    count += 1\n",
    "    if(length > max):\n",
    "        max = length\n",
    "    \n",
    "print(\"Maximum Length \", max)\n",
    "print(\"Avg Length\", total / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Using Stratified sampling, split the data into 70-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake articles in Training set ->  1696\n",
      "Number of Real articles in Training set ->  1494\n",
      "Number of Fake articles in Testing set ->  424\n",
      "Number of Real articles in Testing set ->  374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.pop('Label')\n",
    "x = df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "train_count = y_train.value_counts()\n",
    "test_count = y_test.value_counts()\n",
    "\n",
    "print('Number of Fake articles in Training set -> ', train_count[0])\n",
    "print('Number of Real articles in Training set -> ', train_count[1])\n",
    "print('Number of Fake articles in Testing set -> ', test_count[0])\n",
    "print('Number of Real articles in Testing set -> ', test_count[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "    * Lowercase all the words\n",
    "    * Negation Handling\n",
    "    * Remove Stopwords\n",
    "    * Remove punctuations and Empty Strings from the array\n",
    "* Stemming\n",
    "\n",
    "Source - https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "\n",
    "#### Setup:\n",
    "\n",
    "* Import NLTK\n",
    "* Download and import stopwords from NLTK.corpus\n",
    "* Import PorterStemmer which is the module used for stemming\n",
    "* Import NLTK Vader Sentiment Analysis Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK library and its needed modules\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Import the Vader Sentiment Analysis Library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the Apostrophes connecting words\n",
    "appos_file = open('appos.txt','r')\n",
    "appos = eval(appos_file.read())\n",
    "appos_file.close()\n",
    "\n",
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "# Function to preprocess a single article\n",
    "# Document refers to the text of the Article.\n",
    "def processDocument(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "        words = map(str.lower, words)\n",
    "        \n",
    "        # Negation Handling map is'nt to is not : \n",
    "        words = map(lambda x: negationHandling(x), words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "        \n",
    "        # stemming\n",
    "        words = map(lambda x: ps.stem(x), words)\n",
    "        \n",
    "        # Adding the preprocessed words to the document\n",
    "        tokens = tokens + list(words)\n",
    "        \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of training data\n",
      "Preprocessing Completed for HeadLine of training data\n"
     ]
    }
   ],
   "source": [
    "# Processing the body i.e. text of the Article\n",
    "train_body = x_train.loc[:,'Body']\n",
    "train_raw_body = []\n",
    "train_body_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_body_sentiment.append(list(analyser.polarity_scores(train_body.iloc[i]).values()))\n",
    "    train_raw_body.append(train_body.iloc[i])\n",
    "        \n",
    "train_body_wordArray = list(map(lambda x: processDocument(x), train_raw_body))\n",
    "print(\"Preprocessing Completed for Body of training data\")\n",
    "\n",
    "# Process the Headlines of the training data.\n",
    "train_headline = x_train.loc[:,'Headline']\n",
    "train_raw_headline = []\n",
    "train_headline_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment.append(list(analyser.polarity_scores(train_headline.iloc[i]).values()))\n",
    "    train_raw_headline.append(train_headline.iloc[i])\n",
    "        \n",
    "train_headline_wordArray = list(map(lambda x: processDocument(x), train_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec Model\n",
      "Training iteration 10\n",
      "Training iteration 20\n",
      "Training iteration 30\n",
      "Training iteration 40\n",
      "Training iteration 50\n",
      "Training iteration 60\n",
      "Training iteration 70\n",
      "Training iteration 80\n",
      "Training iteration 90\n",
      "Training iteration 100\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_body_data = [TaggedDocument(\n",
    "    words = train_body_wordArray[i], \n",
    "    tags = [str(i)]) for i, _d in enumerate(train_body_wordArray)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size = vec_size,\n",
    "    alpha = alpha, \n",
    "    min_alpha = 0.025,\n",
    "    min_count = 5,\n",
    "    window = 10,\n",
    "    dm = 1)\n",
    "\n",
    "model.build_vocab(tagged_body_data)\n",
    "print('Training Doc2Vec Model')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if ((epoch + 1) % 10 == 0):\n",
    "        print('Training iteration {0}'.format(epoch + 1))\n",
    "    model.train(tagged_body_data,\n",
    "                total_examples = model.corpus_count,\n",
    "                epochs = model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of testing data\n",
      "Preprocessing Completed for HeadLine of testing data\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the body of the article of test Data set \n",
    "test_body = x_test.loc[:,'Body']\n",
    "test_raw_body = []\n",
    "test_body_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_body_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_body.append(test_body.iloc[i])\n",
    "        \n",
    "test_body_wordArray = list(map(lambda x: processDocument(x), test_raw_body))\n",
    "print(\"Preprocessing Completed for Body of testing data\")\n",
    "\n",
    "# Preprocess the Headline of the article for testing dataset\n",
    "test_headLine = x_test.loc[:,'Headline']\n",
    "test_raw_headline = []\n",
    "test_headline_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_headline.append(test_headLine.iloc[i])\n",
    "        \n",
    "test_headline_wordArray = list(map(lambda x: processDocument(x), test_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Contains the list of Fake News website URL's\n",
    "fake_URL_df = pd.read_csv(\"Data/fake_news_websites.csv\")\n",
    "# Contains the list of Authentic News website URL's\n",
    "fact_URL_df = pd.read_csv(\"Data/fact_news_websites.csv\")\n",
    "\n",
    "##Computes the similarity score between 2 strings\n",
    "def similarityRatio(url_one, url_two):\n",
    "    return SequenceMatcher(None, url_one, url_two).ratio()\n",
    "\n",
    "##Extracts the domain from a URL - for example: 'https://www.bbc.com' will become 'bbc'\n",
    "def getDomain(url):\n",
    "    return url.lstrip(\"https://www.\").split(\".\")[0]\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of fake websites\n",
    "def fakeURLCheckAssign(url):\n",
    "    minSimilarity = 0.5\n",
    "    for i in range(len(fake_URL_df['SiteName'])):\n",
    "        similarity = similarityRatio(getDomain(url),getDomain(fake_URL_df['SiteName'][i]))\n",
    "        if (similarity > 0.75):\n",
    "            minSimilarity = min(minSimilarity,(1 - similarity))\n",
    "            \n",
    "    return minSimilarity\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of authentic websites\n",
    "def factURLCheckAssign(url):\n",
    "    for i in range(len(fact_URL_df['SiteName'])):\n",
    "        minSimilarity = 0.5\n",
    "        similarity = similarityRatio(getDomain(url), getDomain(fact_URL_df['SiteName'][i]))\n",
    "\n",
    "        if similarity == 1:\n",
    "            return similarity\n",
    "            \n",
    "        if similarity > 0.75:\n",
    "            minSimilarity = min(minSimilarity,1 - similarity)\n",
    "            \n",
    "    return minSimilarity\n",
    "          \n",
    "def URLScore(url):\n",
    "    fakeScore = fakeURLCheckAssign(url)\n",
    "    factScore = factURLCheckAssign(url)\n",
    "    if factScore == 1:\n",
    "        return 1\n",
    "    if fakeScore == 0:\n",
    "        return 0\n",
    "    if factScore == 0.5 and fakeScore == 0.5:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return min(fakeScore, factScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_URL = x_test.loc[:,'URLs']\n",
    "train_URL = x_train.loc[:,'URLs']\n",
    "# Higher Score denotes that the article is more authentic\n",
    "# Completely Real Article URL Score - 1 & Completely Fake Article URL Score - 0\n",
    "\n",
    "# Training data set URL Score\n",
    "train_URLScore_vector = []    \n",
    "for i in range(x_train.shape[0]):\n",
    "    train_URLScore_vector.append(URLScore(train_URL.iloc[i]))\n",
    "\n",
    "# Testing data set URL Score\n",
    "test_URLScore_vector = []    \n",
    "for i in range(x_test.shape[0]):\n",
    "    test_URLScore_vector.append(URLScore(test_URL.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word vectors using the trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the np training data (3190, 609)\n",
      "Shape of the np training data (798, 609)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# Training set Body Word Vector  \n",
    "train_body_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    trainBodyConcat = np.concatenate([model.docvecs[i], np.asarray(train_body_sentiment[i])])\n",
    "    train_body_vector.append(trainBodyConcat)\n",
    "\n",
    "# Training data set Headline Word Vectors\n",
    "train_headline_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment[i].append(train_URLScore_vector[i])\n",
    "    trainHeadConcat = np.concatenate([model.infer_vector(train_headline_wordArray[i]), \n",
    "                                      np.asarray(train_headline_sentiment[i])]) \n",
    "    train_headline_vector.append(trainHeadConcat)\n",
    "\n",
    "# Testing set Body Word Vector\n",
    "test_body_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    testBodyConcat = np.concatenate([model.infer_vector(test_body_wordArray[i]), \n",
    "                                     np.asarray(test_body_sentiment[i])])\n",
    "    test_body_vector.append(testBodyConcat)\n",
    "    \n",
    "# Testing set Headline Word Vectors\n",
    "test_headline_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment[i].append(test_URLScore_vector[i])\n",
    "    testHeadConcat = np.concatenate([model.infer_vector(test_headline_wordArray[i]), \n",
    "                                     np.asarray(test_headline_sentiment[i])])\n",
    "    test_headline_vector.append(testHeadConcat)    \n",
    "\n",
    "# Create Numpy Array for training data to train sklearn models\n",
    "np_train_headline = np.array([np.array(xi) for xi in train_headline_vector]) \n",
    "np_train_body = np.array([np.array(xi) for xi in train_body_vector])\n",
    "\n",
    "inp_x_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    inp_x_train.append(np.concatenate((np_train_headline[i], np_train_body[i])))\n",
    "\n",
    "inp_x_train = np.array(inp_x_train)\n",
    "\n",
    "# Create np Array for testing data to train sklearn models\n",
    "np_test_headline = np.array([np.array(xi) for xi in test_headline_vector])\n",
    "np_test_body = np.array([np.array(xi) for xi in test_body_vector])\n",
    "\n",
    "inp_x_test = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    inp_x_test.append(np.concatenate((np_test_headline[i], np_test_body[i])))\n",
    "\n",
    "inp_x_test = np.array(inp_x_test)\n",
    "\n",
    "print('Shape of the np training data', inp_x_train.shape)\n",
    "print('Shape of the np training data', inp_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'C': 0.1, 'kernel': 'linear'}\n",
      "accuracy -> 0.930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "C = [0.1, 0.5, 1, 5, 10, 50]\n",
    "param_grid = [\n",
    "    {'C': C, 'kernel': ['linear']},\n",
    "    {'C': C, 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "    {'degree': [2,3,4], 'kernel': ['poly']},\n",
    "    {'coef0': [0.0], 'kernel': ['sigmoid']} \n",
    "]\n",
    "\n",
    "table = {}\n",
    "\n",
    "score_metric = 'accuracy'\n",
    "clf = GridSearchCV(svm.SVC(), param_grid, cv = 5, scoring = score_metric)\n",
    "clf.fit(inp_x_train, y_train)\n",
    "print(\"Best parameters set found :\", clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "\n",
    "for mean, params in zip(means, clf.cv_results_['params']):\n",
    "    if params == clf.best_params_:\n",
    "        print(\"%s -> %0.3f\" % (score_metric, mean))\n",
    "    key = str(params)\n",
    "    if key not in table:\n",
    "        table[key] = []\n",
    "    table[key].append(\"%0.3f\" % (mean))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Using GridSearchCV to find the optimal parameters for this training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SVC with above parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy ->  0.9987460815047022\n",
      "Accuracy ->  0.8170426065162907\n",
      "Precision ->  0.8639981743145061\n",
      "Recall ->  0.8170426065162907\n",
      "F-Score ->  0.8133759157398764\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "svm_model = svm.SVC(C = 0.1, kernel = 'linear')\n",
    "svm_model.fit(inp_x_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(inp_x_test)\n",
    "y_train_pred = svm_model.predict(inp_x_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print('Train Accuracy -> ', train_accuracy)\n",
    "print('Accuracy -> ', accuracy)\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy ->  0.9987460815047022\n",
      "Accuracy ->  0.5639097744360902\n",
      "Precision ->  0.7438189629918953\n",
      "Recall ->  0.5639097744360902\n",
      "F-Score ->  0.43942470742219686\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(inp_x_train, y_train)\n",
    "\n",
    "nb_y_pred = gnb.predict(inp_x_test)\n",
    "y_train_pred = svm_model.predict(inp_x_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, nb_y_pred, average = 'weighted')\n",
    "\n",
    "print('Train Accuracy -> ', train_accuracy)\n",
    "print('Accuracy -> ', accuracy)\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy ->  0.9987460815047022\n",
      "Accuracy ->  0.949874686716792\n",
      "Precision ->  0.952310672656313\n",
      "Recall ->  0.949874686716792\n",
      "F-Score ->  0.9499232113474533\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(inp_x_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_clf.predict(inp_x_test)\n",
    "y_train_pred = svm_model.predict(inp_x_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, dt_y_pred, average = 'weighted')\n",
    "\n",
    "print('Train Accuracy -> ', train_accuracy)\n",
    "print('Accuracy -> ', accuracy)\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "#### Neural Network\n",
    "\n",
    "* Import Tensorflow\n",
    "* Import Keras\n",
    "* Set the seed value to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.2751 - acc: 0.4558\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2499 - acc: 0.5317\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.2489 - acc: 0.5317\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2445 - acc: 0.5317\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.2233 - acc: 0.5317\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.1973 - acc: 0.7050\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.1789 - acc: 0.8455\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.1660 - acc: 0.8693\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.1554 - acc: 0.8909\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.1471 - acc: 0.9000\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2504 - acc: 0.6520\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1652 - acc: 0.7734\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1206 - acc: 0.8467\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0947 - acc: 0.8831\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0772 - acc: 0.9060\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0654 - acc: 0.9232\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0557 - acc: 0.9354\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0488 - acc: 0.9458\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0434 - acc: 0.9545\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0395 - acc: 0.9621\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2840 - acc: 0.5433\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2202 - acc: 0.6665\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1596 - acc: 0.7630\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.1152 - acc: 0.8382\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0873 - acc: 0.8875\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0711 - acc: 0.9150\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0604 - acc: 0.9310\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0529 - acc: 0.9398\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0475 - acc: 0.9502\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0434 - acc: 0.9536\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2476 - acc: 0.6577\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1454 - acc: 0.8119\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0988 - acc: 0.8774\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0746 - acc: 0.9094\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0602 - acc: 0.9313\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0497 - acc: 0.9455\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0427 - acc: 0.9533\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0379 - acc: 0.9577\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0329 - acc: 0.9649\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0293 - acc: 0.9712\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2880 - acc: 0.5599\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1701 - acc: 0.7740\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1092 - acc: 0.8655\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0817 - acc: 0.9053\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0651 - acc: 0.9276\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0547 - acc: 0.9417\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0482 - acc: 0.9498\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0430 - acc: 0.9571\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0386 - acc: 0.9611\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0359 - acc: 0.9661\n",
      "798/798 [==============================] - 0s 30us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2399 - acc: 0.6643\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1257 - acc: 0.8414\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0839 - acc: 0.8972\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0626 - acc: 0.9245\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0503 - acc: 0.9436\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0420 - acc: 0.9567\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0355 - acc: 0.9639\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0304 - acc: 0.9730\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0266 - acc: 0.9777\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0234 - acc: 0.9824\n",
      "798/798 [==============================] - 0s 30us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2207 - acc: 0.6972\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1088 - acc: 0.8671\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0721 - acc: 0.9160\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0554 - acc: 0.9361\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0448 - acc: 0.9514\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0385 - acc: 0.9608\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0331 - acc: 0.9680\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0292 - acc: 0.9730\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0254 - acc: 0.9771\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0230 - acc: 0.9812\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2468 - acc: 0.6207\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1223 - acc: 0.8423\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0775 - acc: 0.9066\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0566 - acc: 0.9392\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0468 - acc: 0.9498\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0391 - acc: 0.9589\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0333 - acc: 0.9661\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0288 - acc: 0.9740\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0250 - acc: 0.9784\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0218 - acc: 0.9818\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2553 - acc: 0.6154\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1276 - acc: 0.8429\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0844 - acc: 0.9047\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0652 - acc: 0.9273\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0528 - acc: 0.9442\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0446 - acc: 0.9545\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0383 - acc: 0.9627\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0331 - acc: 0.9687\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0292 - acc: 0.9730\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0252 - acc: 0.9793\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2051 - acc: 0.7210\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0947 - acc: 0.8806\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0603 - acc: 0.9285\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0448 - acc: 0.9480\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0364 - acc: 0.9599\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0308 - acc: 0.9696\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0271 - acc: 0.9765\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0213 - acc: 0.9809\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0175 - acc: 0.9865\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0148 - acc: 0.9909\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2184 - acc: 0.7019\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1042 - acc: 0.8715\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0688 - acc: 0.9197\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0521 - acc: 0.9423\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0420 - acc: 0.9527\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0338 - acc: 0.9649\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0299 - acc: 0.9708\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0233 - acc: 0.9796\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0194 - acc: 0.9846\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0172 - acc: 0.9887\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2236 - acc: 0.6875\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0989 - acc: 0.8708\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0645 - acc: 0.9254\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0480 - acc: 0.9439\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0380 - acc: 0.9614\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0310 - acc: 0.9708\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0265 - acc: 0.9774\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0225 - acc: 0.9812\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0190 - acc: 0.9850\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0163 - acc: 0.9884\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2167 - acc: 0.6875\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0951 - acc: 0.8856\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0671 - acc: 0.9226\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0521 - acc: 0.9420\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0421 - acc: 0.9571\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0346 - acc: 0.9639\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0288 - acc: 0.9724\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0242 - acc: 0.9787\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0213 - acc: 0.9837\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0178 - acc: 0.9850\n",
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2274 - acc: 0.6749\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0989 - acc: 0.8781\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0660 - acc: 0.9229\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0488 - acc: 0.9461\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0400 - acc: 0.9552\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0323 - acc: 0.9687\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0272 - acc: 0.9730\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0229 - acc: 0.9806\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0196 - acc: 0.9859\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0169 - acc: 0.9884\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1717 - acc: 0.7696\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0721 - acc: 0.9091\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0495 - acc: 0.9426\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0374 - acc: 0.9614\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0284 - acc: 0.9712\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0231 - acc: 0.9803\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0195 - acc: 0.9850\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0170 - acc: 0.9881\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0142 - acc: 0.9903\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0122 - acc: 0.9925\n",
      "798/798 [==============================] - 0s 30us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2202 - acc: 0.6978\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0890 - acc: 0.8912\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0614 - acc: 0.9266\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0466 - acc: 0.9455\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0367 - acc: 0.9611\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0306 - acc: 0.9705\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0264 - acc: 0.9743\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0234 - acc: 0.9781\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0215 - acc: 0.9809\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0190 - acc: 0.9853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 29us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2059 - acc: 0.7135\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0907 - acc: 0.8871\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0585 - acc: 0.9317\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0423 - acc: 0.9520\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0338 - acc: 0.9665\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0282 - acc: 0.9746\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0234 - acc: 0.9815\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0201 - acc: 0.9859\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0169 - acc: 0.9900\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0140 - acc: 0.9909\n",
      "798/798 [==============================] - 0s 32us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1765 - acc: 0.7602\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0728 - acc: 0.9085\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0490 - acc: 0.9417\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0381 - acc: 0.9577\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0286 - acc: 0.9737\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0230 - acc: 0.9803\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0197 - acc: 0.9850\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0155 - acc: 0.9890\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0129 - acc: 0.9922\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0114 - acc: 0.9931\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2193 - acc: 0.6818\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0848 - acc: 0.8897\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0524 - acc: 0.9395\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0380 - acc: 0.9592\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0294 - acc: 0.9708\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0240 - acc: 0.9790\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0193 - acc: 0.9834\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0158 - acc: 0.9900\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0135 - acc: 0.9912\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0114 - acc: 0.9934\n",
      "798/798 [==============================] - 0s 28us/step\n",
      "{2: 0.8999999983931036, 4: 0.9620689719821965, 6: 0.9536050158234599, 8: 0.9711598785693369, 10: 0.9661442032054673, 12: 0.9824451497729669, 14: 0.9811912319129538, 16: 0.9818181948602013, 18: 0.979310354655813, 20: 0.9909090995788574, 22: 0.9887147405678202, 24: 0.9884012595613174, 26: 0.984952990724749, 28: 0.9884012649799215, 30: 0.9924764962031923, 32: 0.9852664679942834, 34: 0.9909090995788574, 36: 0.9931034548529263, 38: 0.9934169330567028}\n",
      "{2: 0.7919799519660777, 4: 0.814536326212393, 6: 0.8220551235036444, 8: 0.834586457052924, 10: 0.8245614011186108, 12: 0.8408521301764295, 14: 0.8320801955715457, 16: 0.8421052630085096, 18: 0.8233082723199275, 20: 0.8020050195524269, 22: 0.8421052630085096, 24: 0.8395989922652567, 26: 0.8170426009890429, 28: 0.8333333272085452, 30: 0.7568922272899694, 32: 0.8295739248282927, 34: 0.8283208012580872, 36: 0.8145363451842975, 38: 0.8120300744410446}\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "# Set the seed as 7 to get reproducible results\n",
    "seed(7)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed as 7 here as well\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "train_accuracy = {}\n",
    "test_accuracy = {}\n",
    "models = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    no_of_hidden_neurons = i * 2\n",
    "    \n",
    "    # Create a new Model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(no_of_hidden_neurons, input_dim = inp_x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'mse',optimizer = 'adam',metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(inp_x_train, y_train, epochs = 10, batch_size = 100, verbose=2)\n",
    "    train_accuracy[no_of_hidden_neurons] = history.history['acc'][9]\n",
    "    \n",
    "    # Store the model\n",
    "    models.append(model)\n",
    "    \n",
    "    # Calculate the score on the testing data set\n",
    "    test_scores = model.evaluate(inp_x_test, y_test, batch_size = 100)\n",
    "    test_accuracy[no_of_hidden_neurons] = test_scores[1]\n",
    "    # Reset keras and tf\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "print(train_accuracy)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
