{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier\n",
    " \n",
    "Data : https://www.kaggle.com/jruvika/fake-news-detection/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hide deprecated warnings of sklearn package\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake Articles ->  (2120, 4)\n",
      "Number of Real Articles ->  (1868, 4)\n",
      "Maximum Length  32767\n",
      "Avg Length 2941.288365095286\n"
     ]
    }
   ],
   "source": [
    "# Drop the data with null or undefined values\n",
    "df = df.dropna()\n",
    "# Get the number of each label in the data\n",
    "fake = df[df.Label ==  0]\n",
    "real = df[df.Label ==  1]\n",
    "\n",
    "print('Number of Fake Articles -> ', fake.shape)\n",
    "print('Number of Real Articles -> ', real.shape)\n",
    "\n",
    "# Max Count of words in Document - Split by 10,000\n",
    "max = 0\n",
    "total = 0\n",
    "count = 0;\n",
    "for i in range(df.shape[0]):\n",
    "    length = len(df.iloc[i,2])\n",
    "    total += length\n",
    "    count += 1\n",
    "    if(length > max):\n",
    "        max = length\n",
    "    \n",
    "print(\"Maximum Length \", max)\n",
    "print(\"Avg Length\", total / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Using Stratified sampling, split the data into 70-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake articles in Training set ->  1696\n",
      "Number of Real articles in Training set ->  1494\n",
      "Number of Fake articles in Testing set ->  424\n",
      "Number of Real articles in Testing set ->  374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.pop('Label')\n",
    "x = df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "train_count = y_train.value_counts()\n",
    "test_count = y_test.value_counts()\n",
    "\n",
    "print('Number of Fake articles in Training set -> ', train_count[0])\n",
    "print('Number of Real articles in Training set -> ', train_count[1])\n",
    "print('Number of Fake articles in Testing set -> ', test_count[0])\n",
    "print('Number of Real articles in Testing set -> ', test_count[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "    * Lowercase all the words\n",
    "    * Negation Handling\n",
    "    * Remove Stopwords\n",
    "    * Remove punctuations and Empty Strings from the array\n",
    "* Stemming\n",
    "\n",
    "Source - https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "\n",
    "#### Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Load the Apostrophes connecting words\n",
    "appos_file = open('appos.txt','r')\n",
    "appos = eval(appos_file.read())\n",
    "appos_file.close()\n",
    "\n",
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "# Function to preprocess a single article\n",
    "# Document refers to the text of the Article.\n",
    "def processDocument(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "        words = map(str.lower, words)\n",
    "        \n",
    "        # Negation Handling map is'nt to is not : \n",
    "        words = map(lambda x: negationHandling(x), words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "        \n",
    "        # stemming\n",
    "        words = map(lambda x: ps.stem(x), words)\n",
    "        \n",
    "        # Adding the preprocessed words to the document\n",
    "        tokens = tokens + list(words)\n",
    "        \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of training data\n",
      "Preprocessing Completed for HeadLine of training data\n"
     ]
    }
   ],
   "source": [
    "# Processing the body i.e. test of the Article\n",
    "train_Body = x_train.loc[:,'Body']\n",
    "train_docBodyWordArray = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_docBodyWordArray.append(train_Body.iloc[i])\n",
    "        \n",
    "train_BodywordArray = list(map(lambda x: processDocument(x), train_docBodyWordArray))\n",
    "print(\"Preprocessing Completed for Body of training data\")\n",
    "\n",
    "# Process the Headlines of the training data.\n",
    "train_headLine = x_train.loc[:,'Headline']\n",
    "\n",
    "train_docHeadLineWordArray = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_docHeadLineWordArray.append(train_headLine.iloc[i])\n",
    "        \n",
    "train_HeadLineArray = list(map(lambda x: processDocument(x), train_docHeadLineWordArray))\n",
    "print(\"Preprocessing Completed for HeadLine of training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/ipykernel/__main__.py:23: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 9\n",
      "Training iteration 19\n",
      "Training iteration 29\n",
      "Training iteration 39\n",
      "Training iteration 49\n",
      "Training iteration 59\n",
      "Training iteration 69\n",
      "Training iteration 79\n",
      "Training iteration 89\n",
      "Training iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_DocBodyData = [TaggedDocument(words=train_BodywordArray[i], tags=[str(i)]) for i, _d in enumerate(train_BodywordArray)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size = vec_size,\n",
    "    alpha = alpha, \n",
    "    min_alpha = 0.025,\n",
    "    min_count = 5,\n",
    "    window = 10,\n",
    "    dm = 1)\n",
    "\n",
    "model.build_vocab(tagged_DocBodyData)\n",
    "print('Training Doc2Vec Model')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if ((epoch + 1) % 10 == 0):\n",
    "        print('Training iteration {0}'.format(epoch + 1))\n",
    "    model.train(tagged_DocBodyData,total_examples = model.corpus_count, epochs = model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for HeadLine of testing data\n",
      "Preprocessing Completed for Body of testing data\n"
     ]
    }
   ],
   "source": [
    "# Testing Data Body of the article\n",
    "test_headLine = x_test.loc[:,'Headline']\n",
    "\n",
    "test_docHeadLineWordArray = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_docHeadLineWordArray.append(test_headLine.iloc[i])\n",
    "        \n",
    "test_HeadLineArray = list(map(lambda x: processDocument(x), test_docHeadLineWordArray))\n",
    "print(\"Preprocessing Completed for HeadLine of testing data\")\n",
    "\n",
    "# Testing Data Headline of the article\n",
    "test_body = x_test.loc[:,'Body']\n",
    "\n",
    "test_docBodyWordArray = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_docBodyWordArray.append(test_body.iloc[i])\n",
    "        \n",
    "test_BodyArray = list(map(lambda x: processDocument(x), test_docBodyWordArray))\n",
    "print(\"Preprocessing Completed for Body of testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word vectors using the trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the numpy training data (3190, 600)\n",
      "Shape of the numpy training data (798, 600)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# Training set Body Word Vector  \n",
    "train_bodyVector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_bodyVector.append(model.docvecs[i])\n",
    "\n",
    "# Training data set Headline Word Vectors\n",
    "train_headLineVector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headLineVector.append(model.infer_vector(train_HeadLineArray[i]))\n",
    "\n",
    "# Testing set Headline Word Vectors\n",
    "test_headLineVector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headLineVector.append(model.infer_vector(test_HeadLineArray[i]))    \n",
    "\n",
    "# Testing set Body Word Vector\n",
    "test_bodyVector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_bodyVector.append(model.infer_vector(test_BodyArray[i]))\n",
    "    \n",
    "# Create Numpy Array for training data to train sklearn models\n",
    "np_trainHeadline = numpy.array([numpy.array(xi) for xi in train_headLineVector]) \n",
    "np_trainBody = numpy.array([numpy.array(xi) for xi in train_bodyVector])\n",
    "\n",
    "inp_x_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    inp_x_train.append(numpy.concatenate((np_trainHeadline[i], np_trainBody[i])))\n",
    "\n",
    "inp_x_train = numpy.array(inp_x_train)\n",
    "\n",
    "# Create Numpy Array for testing data to train sklearn models\n",
    "np_testHeadline = numpy.array([numpy.array(xi) for xi in test_headLineVector])\n",
    "np_testBody = numpy.array([numpy.array(xi) for xi in test_bodyVector])\n",
    "\n",
    "inp_x_test = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    inp_x_test.append(numpy.concatenate((np_testHeadline[i],np_testBody[i])))\n",
    "\n",
    "inp_x_test = numpy.array(inp_x_test)\n",
    "\n",
    "print('Shape of the numpy training data', inp_x_train.shape)\n",
    "print('Shape of the numpy training data', inp_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Using GridSearchCV to find the optimal parameters for this training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "accuracy -> 0.917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "C = [0.1, 0.5, 1, 5, 10, 50]\n",
    "param_grid = [\n",
    "    {'C': C, 'kernel': ['linear']},\n",
    "    {'C': C, 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "    {'degree': [2,3,4], 'kernel': ['poly']},\n",
    "    {'coef0': [0.0], 'kernel': ['sigmoid']} \n",
    "]\n",
    "\n",
    "table = {}\n",
    "\n",
    "score_metric = 'accuracy'\n",
    "clf = GridSearchCV(svm.SVC(), param_grid, cv = 5, scoring = score_metric)\n",
    "clf.fit(inp_x_train, y_train)\n",
    "print(\"Best parameters set found :\", clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "\n",
    "for mean, params in zip(means, clf.cv_results_['params']):\n",
    "    if params == clf.best_params_:\n",
    "        print(\"%s -> %0.3f\" % (score_metric, mean))\n",
    "    key = str(params)\n",
    "    if key not in table:\n",
    "        table[key] = []\n",
    "    table[key].append(\"%0.3f\" % (mean))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SVC with above parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.7192982456140351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = svm.SVC(C = 10, gamma = 0.0001, kernel = 'rbf')\n",
    "svm_model.fit(inp_x_train, y_train)\n",
    "y_pred = svm_model.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy -> ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.5325814536340853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(inp_x_train, y_train)\n",
    "\n",
    "nb_y_pred = gnb.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "print('Accuracy -> ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.4548872180451128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(inp_x_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_clf.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "print('Accuracy -> ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
